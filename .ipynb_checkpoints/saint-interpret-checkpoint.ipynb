{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to interpret our model using  `Captum` library. \n",
    "\n",
    "We show how to use interpretation hooks to examine and better understand embeddings, sub-embeddings, and attention layers. \n",
    "\n",
    "Note: Before running this tutorial, please use `pip install -r requirements.txt` to install the required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.config import Config\n",
    "from src.models import SaintEncoderDecoderModel\n",
    "\n",
    "from transformers import AutoTokenizer, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ed9458ab784ddd85023226e710a72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7712f9d8f3124348b6ffeea5b7981094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b787f1252d8e4f1ca5de548e97646f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1276f9081c7740f4ab17e93e52ed0657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# settings\n",
    "config_path = \"config/saint.yaml\"\n",
    "config = Config.load(config_path)\n",
    "\n",
    "# configuring saint model \n",
    "encoder_config = BertConfig(**config.encoder_config)\n",
    "decoder_config = BertConfig(**config.decoder_config)\n",
    "\n",
    "encoder_config.max_position_embeddings = config.window_size\n",
    "decoder_config.max_position_embeddings = config.window_size\n",
    "\n",
    "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = './output/saint_francois/model.pt'\n",
    "\n",
    "model = SaintEncoderDecoderModel(encoder_config, decoder_config)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")))\n",
    "model.to(device)\n",
    "\n",
    "# use if cuda enabled on device\n",
    "# model.to(config.device)\n",
    "\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES ON SAINT ARCHITECTURE \n",
    "# Encoder takes sequence of exercise embeddings as queries, keys and values; produces output through repeated self-attention mechanism\n",
    "# Decoder takes sequence of response embeddings as queries, keys and values; then alternately applies self-attention and attention layers to encoder output\n",
    "\n",
    "# overall SAINT\n",
    "# input - sequences of exercise info and response info\n",
    "# ouput - kth user response r_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to perform forward pass of the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is from original BERT SQUAD ex, needs replacing\n",
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a custom forward function that will allow us to access the start and end postitions of our prediction using `position` input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Need to define a custom forward function that allows you to access the bounds of our prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
